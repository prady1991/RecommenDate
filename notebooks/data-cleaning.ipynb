{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd83e17f",
   "metadata": {},
   "source": [
    "### Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a4a26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize \n",
    "import re\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d7244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29aa8c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-nli-mean-tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9eb89ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████| 391/391 [00:00<00:00, 161kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████| 190/190 [00:00<00:00, 205kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████| 3.95k/3.95k [00:00<00:00, 3.91MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 2.00/2.00 [00:00<00:00, 559B/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████| 625/625 [00:00<00:00, 287kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 122/122 [00:00<00:00, 72.9kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████| 438M/438M [00:35<00:00, 12.3MB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████| 53.0/53.0 [00:00<00:00, 26.7kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 57.9kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 1.13MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████| 399/399 [00:00<00:00, 135kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 633kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████| 229/229 [00:00<00:00, 140kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155821f1",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e96f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e2451be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../raw_data/clean_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6f0f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>status</th>\n",
       "      <th>sex</th>\n",
       "      <th>orientation</th>\n",
       "      <th>body_type</th>\n",
       "      <th>diet</th>\n",
       "      <th>drinks</th>\n",
       "      <th>drugs</th>\n",
       "      <th>education</th>\n",
       "      <th>income</th>\n",
       "      <th>job</th>\n",
       "      <th>offspring</th>\n",
       "      <th>pets</th>\n",
       "      <th>smokes</th>\n",
       "      <th>speaks</th>\n",
       "      <th>religion_info</th>\n",
       "      <th>strict</th>\n",
       "      <th>speaks_cleaned</th>\n",
       "      <th>primary_language</th>\n",
       "      <th>number_of_languages</th>\n",
       "      <th>essay0_cleaned</th>\n",
       "      <th>essay1_cleaned</th>\n",
       "      <th>essay2_cleaned</th>\n",
       "      <th>essay3_cleaned</th>\n",
       "      <th>essay4_cleaned</th>\n",
       "      <th>essay5_cleaned</th>\n",
       "      <th>essay6_cleaned</th>\n",
       "      <th>essay7_cleaned</th>\n",
       "      <th>essay8_cleaned</th>\n",
       "      <th>essay9_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>a little extra</td>\n",
       "      <td>no restriction</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>working on college/university</td>\n",
       "      <td>104395</td>\n",
       "      <td>transportation</td>\n",
       "      <td>might want kids</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>english</td>\n",
       "      <td>agnosticism</td>\n",
       "      <td>0</td>\n",
       "      <td>['english']</td>\n",
       "      <td>english</td>\n",
       "      <td>1</td>\n",
       "      <td>would love think kind intellectual either dumb...</td>\n",
       "      <td>currently working international agent freight ...</td>\n",
       "      <td>making people laugh ranting good salting findi...</td>\n",
       "      <td>way look six foot half asian half caucasian mu...</td>\n",
       "      <td>book absurdistan republic mouse men book made ...</td>\n",
       "      <td>food water cell phone shelter</td>\n",
       "      <td>duality humorous thing</td>\n",
       "      <td>trying find someone hang anything except club</td>\n",
       "      <td>new california looking someone wisper secret</td>\n",
       "      <td>want swept foot tired norm want catch coffee b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>average</td>\n",
       "      <td>no restriction</td>\n",
       "      <td>often</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>working on space camp</td>\n",
       "      <td>80000</td>\n",
       "      <td>hospitality / travel</td>\n",
       "      <td>might want kids</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>no</td>\n",
       "      <td>english (fluently), spanish (poorly), french (...</td>\n",
       "      <td>agnosticism</td>\n",
       "      <td>0</td>\n",
       "      <td>['english', 'fluently', 'spanish', 'poorly', '...</td>\n",
       "      <td>english</td>\n",
       "      <td>4</td>\n",
       "      <td>chef mean workaholic love cook regardless whet...</td>\n",
       "      <td>dedicating everyday unbelievable badass</td>\n",
       "      <td>silly ridiculous amonts fun wherever smart as ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>die hard christopher moore fan really watch lo...</td>\n",
       "      <td>delicious porkness glory big as doughboy sinki...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>open share anything</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>available</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>thin</td>\n",
       "      <td>no restriction</td>\n",
       "      <td>socially</td>\n",
       "      <td>rather not say</td>\n",
       "      <td>graduated from masters program</td>\n",
       "      <td>104395</td>\n",
       "      <td>other</td>\n",
       "      <td>rather not say</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>no</td>\n",
       "      <td>english, french, c++</td>\n",
       "      <td>agnosticism</td>\n",
       "      <td>0</td>\n",
       "      <td>['english', 'french', 'c']</td>\n",
       "      <td>english</td>\n",
       "      <td>3</td>\n",
       "      <td>ashamed much writing public text online dating...</td>\n",
       "      <td>make nerdy software musician artist experiment...</td>\n",
       "      <td>improvising different context alternating pres...</td>\n",
       "      <td>large jaw large glass physical thing people co...</td>\n",
       "      <td>okay cultural matrix get specific like crossha...</td>\n",
       "      <td>movement conversation creation contemplation t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>viewing listening dancing talking drinking per...</td>\n",
       "      <td>five year old known boogerman</td>\n",
       "      <td>bright open intense silly ironic critical cari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>thin</td>\n",
       "      <td>veggie</td>\n",
       "      <td>socially</td>\n",
       "      <td>rather not say</td>\n",
       "      <td>working on college/university</td>\n",
       "      <td>20000</td>\n",
       "      <td>student</td>\n",
       "      <td>doesn't want kids</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>no</td>\n",
       "      <td>english, german (poorly)</td>\n",
       "      <td>agnosticism</td>\n",
       "      <td>0</td>\n",
       "      <td>['english', 'german', 'poorly']</td>\n",
       "      <td>english</td>\n",
       "      <td>2</td>\n",
       "      <td>work library go school</td>\n",
       "      <td>reading thing written old dead people</td>\n",
       "      <td>playing synthesizer organizing book according ...</td>\n",
       "      <td>socially awkward best</td>\n",
       "      <td>bataille celine beckett lynch jarmusch r w fas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cat german philosophy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feel inclined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>athletic</td>\n",
       "      <td>no restriction</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>graduated from college/university</td>\n",
       "      <td>104395</td>\n",
       "      <td>artistic / musical / writer</td>\n",
       "      <td>rather not say</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>no</td>\n",
       "      <td>english</td>\n",
       "      <td>agnosticism</td>\n",
       "      <td>0</td>\n",
       "      <td>['english']</td>\n",
       "      <td>english</td>\n",
       "      <td>1</td>\n",
       "      <td>hey going currently vague profile know come so...</td>\n",
       "      <td>work work work work play</td>\n",
       "      <td>creating imagery look http bagsbrown blogspot ...</td>\n",
       "      <td>smile lot inquisitive nature</td>\n",
       "      <td>music band rapper musician moment thee oh see ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  age     status sex orientation       body_type            diet  \\\n",
       "0           0   22     single   m    straight  a little extra  no restriction   \n",
       "1           1   35     single   m    straight         average  no restriction   \n",
       "2           2   38  available   m    straight            thin  no restriction   \n",
       "3           3   23     single   m    straight            thin          veggie   \n",
       "4           4   29     single   m    straight        athletic  no restriction   \n",
       "\n",
       "     drinks           drugs                          education  income  \\\n",
       "0  socially           never      working on college/university  104395   \n",
       "1     often       sometimes              working on space camp   80000   \n",
       "2  socially  rather not say     graduated from masters program  104395   \n",
       "3  socially  rather not say      working on college/university   20000   \n",
       "4  socially           never  graduated from college/university  104395   \n",
       "\n",
       "                           job          offspring                       pets  \\\n",
       "0               transportation    might want kids  likes dogs and likes cats   \n",
       "1         hospitality / travel    might want kids  likes dogs and likes cats   \n",
       "2                        other     rather not say  likes dogs and likes cats   \n",
       "3                      student  doesn't want kids  likes dogs and likes cats   \n",
       "4  artistic / musical / writer     rather not say  likes dogs and likes cats   \n",
       "\n",
       "      smokes                                             speaks religion_info  \\\n",
       "0  sometimes                                            english   agnosticism   \n",
       "1         no  english (fluently), spanish (poorly), french (...   agnosticism   \n",
       "2         no                               english, french, c++   agnosticism   \n",
       "3         no                           english, german (poorly)   agnosticism   \n",
       "4         no                                            english   agnosticism   \n",
       "\n",
       "   strict                                     speaks_cleaned primary_language  \\\n",
       "0       0                                        ['english']          english   \n",
       "1       0  ['english', 'fluently', 'spanish', 'poorly', '...          english   \n",
       "2       0                         ['english', 'french', 'c']          english   \n",
       "3       0                    ['english', 'german', 'poorly']          english   \n",
       "4       0                                        ['english']          english   \n",
       "\n",
       "   number_of_languages                                     essay0_cleaned  \\\n",
       "0                    1  would love think kind intellectual either dumb...   \n",
       "1                    4  chef mean workaholic love cook regardless whet...   \n",
       "2                    3  ashamed much writing public text online dating...   \n",
       "3                    2                             work library go school   \n",
       "4                    1  hey going currently vague profile know come so...   \n",
       "\n",
       "                                      essay1_cleaned  \\\n",
       "0  currently working international agent freight ...   \n",
       "1            dedicating everyday unbelievable badass   \n",
       "2  make nerdy software musician artist experiment...   \n",
       "3              reading thing written old dead people   \n",
       "4                           work work work work play   \n",
       "\n",
       "                                      essay2_cleaned  \\\n",
       "0  making people laugh ranting good salting findi...   \n",
       "1  silly ridiculous amonts fun wherever smart as ...   \n",
       "2  improvising different context alternating pres...   \n",
       "3  playing synthesizer organizing book according ...   \n",
       "4  creating imagery look http bagsbrown blogspot ...   \n",
       "\n",
       "                                      essay3_cleaned  \\\n",
       "0  way look six foot half asian half caucasian mu...   \n",
       "1                                                NaN   \n",
       "2  large jaw large glass physical thing people co...   \n",
       "3                              socially awkward best   \n",
       "4                       smile lot inquisitive nature   \n",
       "\n",
       "                                      essay4_cleaned  \\\n",
       "0  book absurdistan republic mouse men book made ...   \n",
       "1  die hard christopher moore fan really watch lo...   \n",
       "2  okay cultural matrix get specific like crossha...   \n",
       "3  bataille celine beckett lynch jarmusch r w fas...   \n",
       "4  music band rapper musician moment thee oh see ...   \n",
       "\n",
       "                                      essay5_cleaned          essay6_cleaned  \\\n",
       "0                      food water cell phone shelter  duality humorous thing   \n",
       "1  delicious porkness glory big as doughboy sinki...                     NaN   \n",
       "2  movement conversation creation contemplation t...                     NaN   \n",
       "3                                                NaN   cat german philosophy   \n",
       "4                                                NaN                     NaN   \n",
       "\n",
       "                                      essay7_cleaned  \\\n",
       "0      trying find someone hang anything except club   \n",
       "1                                                NaN   \n",
       "2  viewing listening dancing talking drinking per...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                 essay8_cleaned  \\\n",
       "0  new california looking someone wisper secret   \n",
       "1                           open share anything   \n",
       "2                 five year old known boogerman   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "\n",
       "                                      essay9_cleaned  \n",
       "0  want swept foot tired norm want catch coffee b...  \n",
       "1                                                NaN  \n",
       "2  bright open intense silly ironic critical cari...  \n",
       "3                                      feel inclined  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53143c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vecs = model.encode(df.essay0_cleaned[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b8c175d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c711d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a79c1392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7325585]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(\n",
    "    [sentence_vecs[0]],\n",
    "    sentence_vecs[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d403285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'would love think kind intellectual either dumbest smart guy smartest dumb guy say tell difference love talk idea concept forge odd metaphor instead reciting cliche like simularities friend mine house underwater salt mine favorite word salt way weird choice know thing life better metaphor seek make little better everyday productively lazy way got tired tying shoe considered hiring five year old would probably tie shoe decided wear leather shoe dress shoe love really serious really deep conversation really silly stuff willing snap light hearted rant kiss funny able make laugh able bend spoon mind telepathically make smile still work love life cool letting wind blow extra point reading guessing favorite video game hint given yet lastly good attention span'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.essay0_cleaned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed436e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chef mean workaholic love cook regardless whether work love drink eat food probably really bad love around people resemble line love outdoors avid skier snowing tahoe least confident friendly interested acting typical guy time patience rediculous act territorial pissing overall likable easygoing individual adventurous always looking forward new thing hopefully sharing right person'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.essay0_cleaned[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e03efaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfok.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ddda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.education = df.education.fillna('graduated from college/university')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e8643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee908d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e13722",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfok.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d04f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../raw_data/clean_data_0.csv\")\n",
    "df2 = pd.read_csv(\"../raw_data/clean_data_1.csv\")\n",
    "df3 = pd.read_csv(\"../raw_data/clean_data_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed889676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.append(df2).append(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca89ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"Unnamed: 0\": \" \"}, errors=\"raise\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cfa988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.education.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e99052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f2edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e70438e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cce208",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.pets.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e66071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pets=df.pets.fillna('likes dogs and likes cats')\n",
    "\n",
    "df.pets.replace(['likes dogs','likes dogs and has cats','has dogs','has dogs and likes cats','has dogs and has cats','has cats','likes cats'],'likes dogs and likes cats',inplace=True)\n",
    "\n",
    "df.pets.replace(['has dogs and dislikes cats','dislikes cats'],'likes dogs and dislikes cats',inplace=True)\n",
    "\n",
    "df.pets.replace(['dislikes dogs and has cats','dislikes dogs'],'dislikes dogs and likes cats',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38943bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddc08ce",
   "metadata": {},
   "source": [
    "### My topics to clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d67a2ff",
   "metadata": {},
   "source": [
    "essay8- The most private thing I am willing to admit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b81a8",
   "metadata": {},
   "source": [
    "essay9- You should message me if..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4035446",
   "metadata": {},
   "source": [
    "status, diet, ethnicity, last_online, religion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0233b5f0",
   "metadata": {},
   "source": [
    "we have decided to remove last_online"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1413b5",
   "metadata": {},
   "source": [
    "#### status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.status.replace(\"unknown\",\"available\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271778c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['status'] = np.where(df['status'].str.contains('single'), 'available', df['status'])\n",
    "df = pd.concat([pd.get_dummies(df.status, prefix='status', prefix_sep='_'), df], axis = 1)\n",
    "\n",
    "df.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab779e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e7435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['status'] = df['status'].map({'unknown': 'available', 'male': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a567a967",
   "metadata": {},
   "source": [
    "#### diet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43948a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.diet.fillna(\"no restriction\", inplace=True)\n",
    "\n",
    "# create dummy variable: strict (1=strictly following a diet)\n",
    "df['strict'] = 0\n",
    "df.loc[df.diet.str.contains('strictly'), 'strict'] = 1\n",
    "df.loc[df.diet.str.len()==1, 'strict'] = 1\n",
    "\n",
    "# group diets\n",
    "df['diet'] = np.where(df['diet'].str.contains('strictly anything|mostly other|anything|mostly anything|strictly other|other'), 'no restriction', df['diet'])\n",
    "df.loc[df.diet=='no restriction', 'strict'] = 0\n",
    "df['diet'] = np.where(df['diet'].str.contains('mostly vegetarian|strictly vegan|strictly vegetarian|mostly vegan|vegan|vegetarian'), 'veggie', df['diet'])\n",
    "df['diet'] = np.where(df['diet'].str.contains('mostly kosher|strictly kosher|kosher'), 'kosher', df['diet'])\n",
    "df['diet'] = np.where(df['diet'].str.contains('mostly halal|strictly halal|halal'), 'halal', df['diet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794fcbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"status\"]==\"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bec8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"diet\"]==\"veggie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5935faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.diet.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58c7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "174bf2ee",
   "metadata": {},
   "source": [
    "#### ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b7d36",
   "metadata": {},
   "source": [
    "We can split ethnicity into 6 big groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba60510",
   "metadata": {},
   "source": [
    "white,asian,hispanic/latin,black,middle eastern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef783da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['eth_num'] = df.ethnicity.str.len()\n",
    "df[\"ethnicity2\"] = \"race\"\n",
    "df.loc[df.eth_num<2, \"ethnicity2\"] = df.ethnicity.str[0]\n",
    "df.loc[df.eth_num==2, \"ethnicity2\"] = \"biracial\"\n",
    "df.loc[df.eth_num>2, \"ethnicity2\"] = \"multiracial\"\n",
    "df.loc[df.eth_num>2, \"eth_num\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac2a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef02a196",
   "metadata": {},
   "source": [
    "#### religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598e05bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(df.religion.isnull().sum()/len(df))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b71328",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"religion\"] = df[\"religion\"].fillna('agnostic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d88384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.religion.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96764693",
   "metadata": {},
   "source": [
    "##### we will have to try different methods of fill NA in our model\n",
    "such as fill NA as agnostic or other\n",
    "\n",
    "#### we can also try to split the data in different ways such as\n",
    "shrink into 6 categories or \n",
    "split into categories and add column for intesity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7752af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.religion.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255f07e",
   "metadata": {},
   "source": [
    "##### fill NA with agnostic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfagn = df.copy()\n",
    "\n",
    "dfagn[\"religion\"] = df[\"religion\"].fillna(\"agnosticism\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d547b",
   "metadata": {},
   "source": [
    "##### fill NA with other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e264cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfother = df.copy()\n",
    "dfother[\"religion\"] = df[\"religion\"].fillna(\"other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809bcac8",
   "metadata": {},
   "source": [
    "##### add features for seriousness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea495051",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfagn['rel_scale'] = np.where(dfagn['religion'].str.contains(\"very serious\"), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_religion_scale(df):\n",
    "    conditions = [\n",
    "        df['religion'].str.contains(\"very serious\"),\n",
    "        df['religion'].str.contains(\"somewhat serious\"),\n",
    "        df['religion'].str.contains(\"not too serious\"),\n",
    "        df['religion'].str.contains(\"laughing\")]\n",
    "    choices = ['very serious', 'somewhat serious', 'not too serious','laughing']\n",
    "    df['rel_scale'] = np.select(conditions, choices, default='normal')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.religion.fillna(\"other\", inplace=True)\n",
    "\n",
    "# create 1 variable: serious (1=yes, 0=neutral, -1=no)\n",
    "df[\"serious\"] = 0\n",
    "df.loc[df.religion.str.contains(\"very|somewhat\"), \"serious\"] = 1\n",
    "df.loc[df.religion.str.contains(\"laughing\"), \"serious\"] = -1\n",
    "df.religion = df.religion.str.split().str[0]\n",
    "# df.groupby(\"religion\")[\"serious\"].mean()\n",
    "\n",
    "# seriousness by religion\n",
    "plt.plot(df.groupby(\"religion\")[\"serious\"].mean(), marker='o', color=\"#5e3a98\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"how seriously does a user take religion?\")\n",
    "plt.ylabel(\"average seriousness\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7717a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfagn = create_religion_scale(dfagn)\n",
    "dfagn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba186594",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfother = create_religion_scale(dfother)\n",
    "\n",
    "dfother"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c98ac3",
   "metadata": {},
   "source": [
    "#### cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean (text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' ') # Remove Punctuation\n",
    "    lowercased = text.lower() # Lower Case\n",
    "    tokenized = word_tokenize(lowercased) # Tokenize\n",
    "    words_only = [word for word in tokenized if word.isalpha()] # Remove numbers\n",
    "    stop_words = set(stopwords.words('english')) # Make stopword list\n",
    "    without_stopwords = [word for word in words_only if not word in stop_words] # Remove Stop Words\n",
    "    lemma=WordNetLemmatizer() # Initiate Lemmatizer\n",
    "    lemmatized = [lemma.lemmatize(word) for word in without_stopwords] # Lemmatize\n",
    "    return lemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['essay0']=df['essay8'].fillna('')\n",
    "df['essay1']=df['essay9'].fillna('')\n",
    "df['essay2']=df['essay8'].fillna('')\n",
    "df['essay3']=df['essay9'].fillna('')\n",
    "df['essay4']=df['essay8'].fillna('')\n",
    "df['essay5']=df['essay9'].fillna('')\n",
    "df['essay6']=df['essay8'].fillna('')\n",
    "df['essay7']=df['essay9'].fillna('')\n",
    "df['essay8']=df['essay8'].fillna('')\n",
    "df['essay9']=df['essay9'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf50be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['essay0clean'] = df['essay0'].apply(lambda x: clean(x)).apply(lambda x:' '.join(x))\n",
    "df['essay1clean'] = df['essay1'].apply(lambda x: clean(x)).apply(lambda x:' '.join(x))\n",
    "df['essay2clean'] = df['essay2'].apply(lambda x: clean(x)).apply(lambda x:' '.join(x))\n",
    "df['essay3clean'] = df['essay3'].apply(lambda x: clean(x)).apply(lambda x:' '.join(x))\n",
    "df['essay4clean'] = df['essay4'].apply(lambda x: clean(x)).apply(lambda x:' '.join(x))\n",
    "df['essay5clean'] = df['essay5'].apply(lambda x: clean(x)).apply(lambda x:' '.join(x))\n",
    "df['essay6clean'] = df['essay6'].apply(lambda x: clean(x)).apply(lambda x:' '.join(x))\n",
    "df['essay7clean'] = df['essay7'].apply(lambda x: clean(x)).apply(lambda x:' '.join(x))\n",
    "df['essay8clean'] = df['essay8'].apply(lambda x: clean(x)).apply(lambda x:' '.join(x))\n",
    "df['essay9clean'] = df['essay9'].apply(lambda x: clean(x)).apply(lambda x:' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e897ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfessay = df[['essay0clean','essay1clean','essay2clean','essay3clean','essay4clean','essay5clean','essay6clean','essay7clean','essay8clean','essay9clean']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ae7461",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfessay.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c3a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dddc341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5,ngram_range=(3,3))\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(dfessay[\"essay0clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2,perplexity=40,random_state=42)\n",
    "tsne_essays = tsne.fit_transform(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['essay8clean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daba436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['essay8'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc8cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['essay9clean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c229a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['essay9'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88e600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7d04ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rohito/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/rohito/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/rohito/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/rohito/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from RecommenDate import clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6245ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_data.clean_data(dfok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50ea1823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>status</th>\n",
       "      <th>sex</th>\n",
       "      <th>orientation</th>\n",
       "      <th>body_type</th>\n",
       "      <th>diet</th>\n",
       "      <th>drinks</th>\n",
       "      <th>drugs</th>\n",
       "      <th>education</th>\n",
       "      <th>income</th>\n",
       "      <th>job</th>\n",
       "      <th>offspring</th>\n",
       "      <th>pets</th>\n",
       "      <th>smokes</th>\n",
       "      <th>speaks</th>\n",
       "      <th>religion_info</th>\n",
       "      <th>strict</th>\n",
       "      <th>speaks_cleaned</th>\n",
       "      <th>primary_language</th>\n",
       "      <th>number_of_languages</th>\n",
       "      <th>essay0_cleaned</th>\n",
       "      <th>essay1_cleaned</th>\n",
       "      <th>essay2_cleaned</th>\n",
       "      <th>essay3_cleaned</th>\n",
       "      <th>essay4_cleaned</th>\n",
       "      <th>essay5_cleaned</th>\n",
       "      <th>essay6_cleaned</th>\n",
       "      <th>essay7_cleaned</th>\n",
       "      <th>essay8_cleaned</th>\n",
       "      <th>essay9_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>a little extra</td>\n",
       "      <td>no restriction</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>working on college/university</td>\n",
       "      <td>104395</td>\n",
       "      <td>transportation</td>\n",
       "      <td>might want kids</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>english</td>\n",
       "      <td>agnosticism</td>\n",
       "      <td>0</td>\n",
       "      <td>[english]</td>\n",
       "      <td>english</td>\n",
       "      <td>1</td>\n",
       "      <td>would love think kind intellectual either dumb...</td>\n",
       "      <td>currently working international agent freight ...</td>\n",
       "      <td>making people laugh ranting good salting findi...</td>\n",
       "      <td>way look six foot half asian half caucasian mu...</td>\n",
       "      <td>book absurdistan republic mouse men book made ...</td>\n",
       "      <td>food water cell phone shelter</td>\n",
       "      <td>duality humorous thing</td>\n",
       "      <td>trying find someone hang anything except club</td>\n",
       "      <td>new california looking someone wisper secret</td>\n",
       "      <td>want swept foot tired norm want catch coffee b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>average</td>\n",
       "      <td>no restriction</td>\n",
       "      <td>often</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>working on space camp</td>\n",
       "      <td>80000</td>\n",
       "      <td>hospitality / travel</td>\n",
       "      <td>might want kids</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>no</td>\n",
       "      <td>english (fluently), spanish (poorly), french (...</td>\n",
       "      <td>agnosticism</td>\n",
       "      <td>0</td>\n",
       "      <td>[english, fluently, spanish, poorly, french, p...</td>\n",
       "      <td>english</td>\n",
       "      <td>4</td>\n",
       "      <td>chef mean workaholic love cook regardless whet...</td>\n",
       "      <td>dedicating everyday unbelievable badass</td>\n",
       "      <td>silly ridiculous amonts fun wherever smart as ...</td>\n",
       "      <td></td>\n",
       "      <td>die hard christopher moore fan really watch lo...</td>\n",
       "      <td>delicious porkness glory big as doughboy sinki...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>open share anything</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>available</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>thin</td>\n",
       "      <td>no restriction</td>\n",
       "      <td>socially</td>\n",
       "      <td>rather not say</td>\n",
       "      <td>graduated from masters program</td>\n",
       "      <td>104395</td>\n",
       "      <td>other</td>\n",
       "      <td>rather not say</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>no</td>\n",
       "      <td>english, french, c++</td>\n",
       "      <td>agnosticism</td>\n",
       "      <td>0</td>\n",
       "      <td>[english, french, c]</td>\n",
       "      <td>english</td>\n",
       "      <td>3</td>\n",
       "      <td>ashamed much writing public text online dating...</td>\n",
       "      <td>make nerdy software musician artist experiment...</td>\n",
       "      <td>improvising different context alternating pres...</td>\n",
       "      <td>large jaw large glass physical thing people co...</td>\n",
       "      <td>okay cultural matrix get specific like crossha...</td>\n",
       "      <td>movement conversation creation contemplation t...</td>\n",
       "      <td></td>\n",
       "      <td>viewing listening dancing talking drinking per...</td>\n",
       "      <td>five year old known boogerman</td>\n",
       "      <td>bright open intense silly ironic critical cari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>thin</td>\n",
       "      <td>veggie</td>\n",
       "      <td>socially</td>\n",
       "      <td>rather not say</td>\n",
       "      <td>working on college/university</td>\n",
       "      <td>20000</td>\n",
       "      <td>student</td>\n",
       "      <td>doesn't want kids</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>no</td>\n",
       "      <td>english, german (poorly)</td>\n",
       "      <td>agnosticism</td>\n",
       "      <td>0</td>\n",
       "      <td>[english, german, poorly]</td>\n",
       "      <td>english</td>\n",
       "      <td>2</td>\n",
       "      <td>work library go school</td>\n",
       "      <td>reading thing written old dead people</td>\n",
       "      <td>playing synthesizer organizing book according ...</td>\n",
       "      <td>socially awkward best</td>\n",
       "      <td>bataille celine beckett lynch jarmusch r w fas...</td>\n",
       "      <td></td>\n",
       "      <td>cat german philosophy</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>feel inclined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>athletic</td>\n",
       "      <td>no restriction</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>graduated from college/university</td>\n",
       "      <td>104395</td>\n",
       "      <td>artistic / musical / writer</td>\n",
       "      <td>rather not say</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>no</td>\n",
       "      <td>english</td>\n",
       "      <td>agnosticism</td>\n",
       "      <td>0</td>\n",
       "      <td>[english]</td>\n",
       "      <td>english</td>\n",
       "      <td>1</td>\n",
       "      <td>hey going currently vague profile know come so...</td>\n",
       "      <td>work work work work play</td>\n",
       "      <td>creating imagery look http bagsbrown blogspot ...</td>\n",
       "      <td>smile lot inquisitive nature</td>\n",
       "      <td>music band rapper musician moment thee oh see ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     status sex orientation       body_type            diet    drinks  \\\n",
       "0   22     single   m    straight  a little extra  no restriction  socially   \n",
       "1   35     single   m    straight         average  no restriction     often   \n",
       "2   38  available   m    straight            thin  no restriction  socially   \n",
       "3   23     single   m    straight            thin          veggie  socially   \n",
       "4   29     single   m    straight        athletic  no restriction  socially   \n",
       "\n",
       "            drugs                          education  income  \\\n",
       "0           never      working on college/university  104395   \n",
       "1       sometimes              working on space camp   80000   \n",
       "2  rather not say     graduated from masters program  104395   \n",
       "3  rather not say      working on college/university   20000   \n",
       "4           never  graduated from college/university  104395   \n",
       "\n",
       "                           job          offspring                       pets  \\\n",
       "0               transportation    might want kids  likes dogs and likes cats   \n",
       "1         hospitality / travel    might want kids  likes dogs and likes cats   \n",
       "2                        other     rather not say  likes dogs and likes cats   \n",
       "3                      student  doesn't want kids  likes dogs and likes cats   \n",
       "4  artistic / musical / writer     rather not say  likes dogs and likes cats   \n",
       "\n",
       "      smokes                                             speaks religion_info  \\\n",
       "0  sometimes                                            english   agnosticism   \n",
       "1         no  english (fluently), spanish (poorly), french (...   agnosticism   \n",
       "2         no                               english, french, c++   agnosticism   \n",
       "3         no                           english, german (poorly)   agnosticism   \n",
       "4         no                                            english   agnosticism   \n",
       "\n",
       "   strict                                     speaks_cleaned primary_language  \\\n",
       "0       0                                          [english]          english   \n",
       "1       0  [english, fluently, spanish, poorly, french, p...          english   \n",
       "2       0                               [english, french, c]          english   \n",
       "3       0                          [english, german, poorly]          english   \n",
       "4       0                                          [english]          english   \n",
       "\n",
       "   number_of_languages                                     essay0_cleaned  \\\n",
       "0                    1  would love think kind intellectual either dumb...   \n",
       "1                    4  chef mean workaholic love cook regardless whet...   \n",
       "2                    3  ashamed much writing public text online dating...   \n",
       "3                    2                             work library go school   \n",
       "4                    1  hey going currently vague profile know come so...   \n",
       "\n",
       "                                      essay1_cleaned  \\\n",
       "0  currently working international agent freight ...   \n",
       "1            dedicating everyday unbelievable badass   \n",
       "2  make nerdy software musician artist experiment...   \n",
       "3              reading thing written old dead people   \n",
       "4                           work work work work play   \n",
       "\n",
       "                                      essay2_cleaned  \\\n",
       "0  making people laugh ranting good salting findi...   \n",
       "1  silly ridiculous amonts fun wherever smart as ...   \n",
       "2  improvising different context alternating pres...   \n",
       "3  playing synthesizer organizing book according ...   \n",
       "4  creating imagery look http bagsbrown blogspot ...   \n",
       "\n",
       "                                      essay3_cleaned  \\\n",
       "0  way look six foot half asian half caucasian mu...   \n",
       "1                                                      \n",
       "2  large jaw large glass physical thing people co...   \n",
       "3                              socially awkward best   \n",
       "4                       smile lot inquisitive nature   \n",
       "\n",
       "                                      essay4_cleaned  \\\n",
       "0  book absurdistan republic mouse men book made ...   \n",
       "1  die hard christopher moore fan really watch lo...   \n",
       "2  okay cultural matrix get specific like crossha...   \n",
       "3  bataille celine beckett lynch jarmusch r w fas...   \n",
       "4  music band rapper musician moment thee oh see ...   \n",
       "\n",
       "                                      essay5_cleaned          essay6_cleaned  \\\n",
       "0                      food water cell phone shelter  duality humorous thing   \n",
       "1  delicious porkness glory big as doughboy sinki...                           \n",
       "2  movement conversation creation contemplation t...                           \n",
       "3                                                      cat german philosophy   \n",
       "4                                                                              \n",
       "\n",
       "                                      essay7_cleaned  \\\n",
       "0      trying find someone hang anything except club   \n",
       "1                                                      \n",
       "2  viewing listening dancing talking drinking per...   \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                                 essay8_cleaned  \\\n",
       "0  new california looking someone wisper secret   \n",
       "1                           open share anything   \n",
       "2                 five year old known boogerman   \n",
       "3                                                 \n",
       "4                                                 \n",
       "\n",
       "                                      essay9_cleaned  \n",
       "0  want swept foot tired norm want catch coffee b...  \n",
       "1                                                     \n",
       "2  bright open intense silly ironic critical cari...  \n",
       "3                                      feel inclined  \n",
       "4                                                     "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5545018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rather not say       35561\n",
       "might want kids      15727\n",
       "doesn't want kids     4776\n",
       "wants more kids       3882\n",
       "Name: offspring, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.offspring.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.offspring.replace([\"doesn't have kids, and doesn't want any\",\"has kids, but doesn't want more\",\"has a kid, but doesn't want more\"],\"doesn't want kids\",inplace=True)\n",
    "clean_df.offspring.replace([\"doesn't have kids\",\"doesn't have kids, but might want them\",\"has kids\",\"has a kid\",\"has a kid, and might want more\",\"might want kids\",\"has kids, and might want more\"],\"might want kids\",inplace=True)\n",
    "clean_df.offspring.replace([\"doesn't have kids, but wants them\",\"wants kids\",\"has a kid, and wants more\",\"has kids, and wants more\"],\"wants more kids\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b922a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.offspring.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3cf803a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Makefile     \u001b[0m\u001b[01;34mnotebooks\u001b[0m/  README.md      requirements.txt  setup.py\r\n",
      "MANIFEST.in  \u001b[01;34mraw_data\u001b[0m/   \u001b[01;34mRecommenDate\u001b[0m/  \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a2c97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  \n",
    "filepath = Path('../raw_data/clean_data.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "clean_df.to_csv(filepath) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "6ee20c6e0097167fdea31101eb44251aeb7bbb6545b9f813d82369e268ae541a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
